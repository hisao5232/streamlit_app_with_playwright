import asyncio
import os
import requests
from playwright.async_api import async_playwright
from datetime import datetime
from zoneinfo import ZoneInfo

# === ç’°å¢ƒå¤‰æ•° ===
API_URL = os.getenv("API_URL")
API_TOKEN = os.getenv("API_TOKEN")

# === APIã¸ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é€ä¿¡ã™ã‚‹é–¢æ•° ===
def save_to_api(source, articles):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ FastAPI çµŒç”±ã§ PostgreSQL ã«ç™»éŒ²ã™ã‚‹ã€‚
    """
    headers = {"Authorization": f"Bearer {API_TOKEN}"}

    # æ—¥æœ¬æ™‚é–“ â†’ UTC â†’ naiveï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‰Šé™¤ï¼‰
    now = datetime.now(ZoneInfo("Asia/Tokyo")).astimezone(ZoneInfo("UTC")).replace(tzinfo=None)
    # JSONåŒ–ã§ãã‚‹ã‚ˆã†ã«ISO8601æ–‡å­—åˆ—ã«å¤‰æ›
    now_str = now.isoformat()

    for title, url in articles:
        payload = {
            "source": source,
            "title": title,
            "url": url,
            "scraped_at": now_str,
        }
        try:
            response = requests.post(API_URL, json=payload, headers=headers)
            if response.status_code == 200:
                print(f"âœ… ç™»éŒ²æˆåŠŸ: {title}")
            else:
                print(f"âŒ ç™»éŒ²å¤±æ•— ({response.status_code}): {title}")
        except Exception as e:
            print(f"âš ï¸ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

# === å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ===
async def scrape_nikkei(page):
    await page.goto("https://business.nikkei.com/ranking/?i_cid=nbpnb_ranking", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('section.p-articleList_item')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.locator('h3.p-articleList_item_title').inner_text()
            href = await article.locator('a.p-articleList_item_link').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://business.nikkei.com" + href
            results.append((title.strip(), href))
        except:
            continue
    return results

async def scrape_yahoo(page):
    await page.goto("https://news.yahoo.co.jp/categories/business", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('a.sc-1nhdoj2-1')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.inner_text()
            url = await article.get_attribute('href')
            if url and title:
                results.append((title.strip(), url))
        except:
            continue
    return results

async def scrape_toyokeizai(page):
    await page.goto("https://toyokeizai.net/list/genre/market", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('li.wd217')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.locator('span.title').inner_text()
            href = await article.locator('span.title > a').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://toyokeizai.net" + href
            results.append((title.strip(), href))
        except:
            continue
    return results

# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        nikkei_page = await browser.new_page()
        yahoo_page = await browser.new_page()
        toyokeizai_page = await browser.new_page()

        # ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        nikkei_task = scrape_nikkei(nikkei_page)
        yahoo_task = scrape_yahoo(yahoo_page)
        toyokeizai_task = scrape_toyokeizai(toyokeizai_page)

        nikkei_news, yahoo_news, toyokeizai_news = await asyncio.gather(
            nikkei_task, yahoo_task, toyokeizai_task
        )

        await browser.close()

        # === APIçµŒç”±ã§ä¿å­˜ ===
        save_to_api("nikkei", nikkei_news)
        save_to_api("yahoo", yahoo_news)
        save_to_api("toyokeizai", toyokeizai_news)

        # === ç¢ºèªç”¨å‡ºåŠ› ===
        print("\nğŸ“° æ—¥çµŒæ–°è çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
        for i, (title, url) in enumerate(nikkei_news, 1):
            print(f"{i}. {title}\n   {url}")

        print("\nğŸ—ï¸ Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ çµŒæ¸ˆ")
        for i, (title, url) in enumerate(yahoo_news, 1):
            print(f"{i}. {title}\n   {url}")

        print("\nğŸ—ï¸ æ±æ´‹çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
        for i, (title, url) in enumerate(toyokeizai_news, 1):
            print(f"{i}. {title}\n   {url}")

# å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(main())
