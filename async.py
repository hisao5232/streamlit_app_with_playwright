import asyncio
from playwright.async_api import async_playwright

async def scrape_nikkei(page):
    await page.goto("https://business.nikkei.com/ranking/?i_cid=nbpnb_ranking", timeout=60000)
    results = []
    articles = await page.locator('section.p-articleList_item').all()

        # æœ€å¤§10ä»¶ã¾ã§ãƒ«ãƒ¼ãƒ—
    for article in articles[:10]:

        try:
            title = await article.locator('h3.p-articleList_item_title').inner_text()
            href = await article.locator('a.p-articleList_item_link').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://business.nikkei.com" + href
            results.append((title.strip(), href))
        except:
            continue

    return results

async def scrape_yahoo(page):
    await page.goto("https://news.yahoo.co.jp/categories/business", timeout=60000)
    results = []
    articles = await page.locator('a.sc-1nhdoj2-1').all()

    for article in articles:
        try:
            title = await article.inner_text()
            url = await article.get_attribute('href')
            if url and title:
                results.append((title.strip(), url))
        except:
            continue

    return results

async def scrape_toyokeizai(page):
    await page.goto("https://toyokeizai.net/list/genre/market", timeout=60000)
    results = []

    # è¨˜äº‹ã®locatorã‚’ã™ã¹ã¦å–å¾—
    articles = await page.locator('li.wd217').all()
    # æœ€å¤§10ä»¶ã¾ã§ãƒ«ãƒ¼ãƒ—
    for article in articles[:10]:
        try:
            title = await article.locator('span.title').inner_text()
            href = await article.locator('span.title > a').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://toyokeizai.net" + href
            results.append((title.strip(), href))
        except:
            continue

    return results


async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        nikkei_page = await browser.new_page()
        yahoo_page = await browser.new_page()
        toyokeizai_page = await browser.new_page()

        # ä¸¡æ–¹ã‚’åŒæ™‚ã«å–å¾—
        nikkei_task = scrape_nikkei(nikkei_page)
        yahoo_task = scrape_yahoo(yahoo_page)
        toyokeizai_task = scrape_toyokeizai(toyokeizai_page)

        nikkei_news, yahoo_news, toyokeizai_news = await asyncio.gather(nikkei_task, yahoo_task, toyokeizai_task)

        await browser.close()

        print("\nğŸ“° æ—¥çµŒæ–°è çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
        for i, (title, url) in enumerate(nikkei_news, 1):
            print(f"{i}. {title}\n   {url}")

        print("\nğŸ—ï¸ Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ çµŒæ¸ˆ")
        for i, (title, url) in enumerate(yahoo_news, 1):
            print(f"{i}. {title}\n   {url}")

        print("\nğŸ—ï¸ æ±æ´‹çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
        for i, (title, url) in enumerate(toyokeizai_news, 1):
            print(f"{i}. {title}\n   {url}")

# å®Ÿè¡Œ
asyncio.run(main())
